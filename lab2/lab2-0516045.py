# -*- coding: utf-8 -*-
"""lab2-0516045.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gZpSRFnz72lBONukBIJ5B3BoPhrxPRFP
"""

import re
import math
import pandas as pd
import string
import nltk
import spacy
import numpy as np
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk import ngrams
from nltk import pos_tag

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def get_corpus():
  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv')
  content_part1 = df.content.to_list()
  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv')
  title_part2 = df.title.to_list()
  content_part2 = df.content.to_list()
  return content_part1, title_part2, content_part2

def get_twograms(document):
  proper_noun_twograms = []
  for content in document:
    content = content.replace('’', '\'')
    content = content.replace('‘', '\'')
    content = content.replace('”', '"')
    content = content.replace('“', '"')
    words = word_tokenize(content)
    pos = pos_tag(words)
    twograms = ngrams(words, 2)
    twograms = list(twograms)
    for i in range(len(words)-1):
      if (pos[i][1] == 'NNP' or pos[i][1] == 'NNPS') and (pos[i+1][1] == 'NNP' or pos[i+1][1] == 'NNPS'):
        proper_noun_twograms.append(twograms[i])
  return proper_noun_twograms

def tokenize(corpus):
    all_token = []
    for content in corpus:
        words = []
        if isinstance(content, str):
            content = content.replace('’', '\'')
            content = content.replace('‘', '\'')
            content = content.replace('”', '"')
            content = content.replace('“', '"')
            doc = nlp(content)
            for token in doc:
                if not token.is_stop and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE':
                    words.append(token.lemma_ + '_' + token.pos_)
        all_token.append(words)
    return all_token

def get_terms(token_list):
    vocabulary = Counter()
    for tokens in token_list:
        vocabulary.update(tokens)    
    return vocabulary

def get_idf(token_list):
    idf = []
    for token, freq in features_terms:
        count = 0
        for words in token_list:
            if token in words:
                count += 1
        idf.append(count)

    for i in range(len(idf)):
        idf[i] = math.log(len(token_list)/idf[i])
    return idf

def doc2vec(doc):
    tmp = []
    tmp.append(doc)
    words = tokenize(tmp)
    tokens = get_terms(words)
    count = 0
    tf_idf = idf.copy()
    for token, freq in features_terms:
        if len(words) == 0:
              tf_idf[count] = 0
        else:
              tf_idf[count] = tokens[token] * tf_idf[count] / len(words)
        count += 1
    return tf_idf

def cosine_similarity(vec_a, vec_b):
  assert len(vec_a) == len(vec_b)
  if sum(vec_a) == 0 or sum(vec_b) == 0:
    return 0 # hack
  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))
  a_2 = sum([i*i for i in vec_a])
  b_2 = sum([i*i for i in vec_b])
  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))

def doc_similarity(doc_a, doc_b):
  return cosine_similarity(doc2vec(doc_a), doc2vec(doc_b))

def k_similar(seed_id, k):
  seed_doc = content_part2[seed_id]
  print('> "{}"'.format(title_part2[seed_id]))
  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(content_part2)]
  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:]
  nearest = [[title_part2[id], similarities[id]] for id in top_indices]
  print()
  for story in reversed(nearest):
    print('* "{}" ({})'.format(story[0], story[1]))

"""DATA NEEDED"""

content_part1, title_part2, content_part2 = get_corpus()

"""PART 1"""

proper_noun_twograms = get_twograms(content_part1)
print(Counter(proper_noun_twograms).most_common(5))

"""PART 2"""

nlp = spacy.load("en_core_web_sm")
all_token = tokenize(content_part2)
features_terms = get_terms(all_token).most_common(512)
idf = get_idf(all_token)
k_similar(45, 5)

