{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3-0516045.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGbXQMg1qTri"
      },
      "source": [
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxJDM-vZzrij"
      },
      "source": [
        "def tokenize(data):\n",
        "    text = []\n",
        "    for i in range(len(data)):\n",
        "        tmp = tknzr.tokenize(data['text'][i].lower())\n",
        "        tmp = ['<s>'] + tmp + ['</s>']\n",
        "        text.append(tmp)\n",
        "    return text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i391pVDVHKQ6"
      },
      "source": [
        "def get_bigrams(text_list):\n",
        "    new_text_list = []\n",
        "    for text in text_list:\n",
        "        new_text_list.append(list(nltk.bigrams(text)))\n",
        "    return new_text_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_6egL4tw0Ii"
      },
      "source": [
        "def get_terms(text_list):\n",
        "    vocabulary = Counter()\n",
        "    for text in text_list:\n",
        "        vocabulary.update(text)\n",
        "    vocabulary = Counter(el for el in vocabulary.elements() if vocabulary[el] >= 3)\n",
        "    return vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgDssOHR0kkp"
      },
      "source": [
        "def replace_UNK(text_list):\n",
        "    valid_tokens = 0\n",
        "    new_text_list = []\n",
        "    for text in text_list:\n",
        "        new_text = []\n",
        "        for i in range(len(text)):\n",
        "            if text[i] in list(vocabulary):\n",
        "                new_text.append(text[i])\n",
        "            else:\n",
        "                new_text.append('<UNK>')\n",
        "        new_text_list.append(new_text)\n",
        "    return new_text_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyt6oz_fi3a_"
      },
      "source": [
        "def get_dict():\n",
        "    for bigrams in training_text_bigrams:\n",
        "        for w1, w2 in bigrams:\n",
        "            counts[w1][w2] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7s5i5iKlUT9"
      },
      "source": [
        "def get_reverse_dict():\n",
        "    for bigrams in training_text_bigrams:\n",
        "        for w2, w1 in bigrams:\n",
        "            reverse_counts[w1][w2] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBZ4Mmjn0E3q"
      },
      "source": [
        "def get_average_perplexity():\n",
        "    training_perplexity = 0\n",
        "    for sentence in training_text_bigrams:\n",
        "        training_probabilities = [(1 + counts[w1][w2])/(sum(counts[w1].values()) + len(vocabulary)) for w1, w2 in sentence]\n",
        "        N = len(sentence)\n",
        "        cross_entropy = -1/N * sum([math.log(p, 2) for p in training_probabilities])\n",
        "        training_perplexity += math.pow(2, cross_entropy)\n",
        "    print(\"average training perplexity:\",training_perplexity / len(training_text_bigrams))\n",
        "    testing_perplexity = 0\n",
        "    for sentence in testing_text_bigrams:\n",
        "        testing_probabilities = [(1 + counts[w1][w2])/(sum(counts[w1].values()) + len(vocabulary)) for w1, w2 in sentence]\n",
        "        N = len(sentence)\n",
        "        cross_entropy = -1/N * sum([math.log(p, 2) for p in testing_probabilities])\n",
        "        testing_perplexity += math.pow(2, cross_entropy)\n",
        "    print(\"average testing perplexity: \",testing_perplexity / len(testing_text_bigrams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9ZO6v3p0FAI"
      },
      "source": [
        "def get_best_average_perplexity():\n",
        "    best_testing_average_perplexity = 1e9\n",
        "    best_gamma = 0\n",
        "    for gamma in gamma_list:\n",
        "        testing_perplexity = 0\n",
        "        for sentence in testing_text_bigrams:\n",
        "            testing_probabilities = [(gamma * (1 + counts[fwd[0]][fwd[1]])/(sum(counts[fwd[0]].values()) + len(vocabulary)) \n",
        "                                      + (1-gamma) * (1 + reverse_counts[bkd[1]][bkd[0]])/(sum(reverse_counts[bkd[1]].values()) + len(vocabulary))) for fwd, bkd in zip(sentence[:-1], sentence[1:])]\n",
        "            N = len(sentence)\n",
        "            cross_entropy = -1/N * sum([math.log(p, 2) for p in testing_probabilities])\n",
        "            testing_perplexity += math.pow(2, cross_entropy)\n",
        "        testing_average_perplexity = testing_perplexity / len(testing_text_bigrams)\n",
        "        if testing_average_perplexity < best_testing_average_perplexity:\n",
        "            best_testing_average_perplexity = testing_average_perplexity\n",
        "            best_gamma = gamma\n",
        "    training_perplexity = 0\n",
        "    for sentence in training_text_bigrams:\n",
        "        training_probabilities = [(best_gamma * (1 + counts[fwd[0]][fwd[1]])/(sum(counts[fwd[0]].values()) + len(vocabulary)) + (1-best_gamma) * (1 + reverse_counts[bkd[1]][bkd[0]])/(sum(reverse_counts[bkd[1]].values()) + len(vocabulary))) for fwd, bkd in zip(sentence[:-1], sentence[1:])]\n",
        "        N = len(sentence)\n",
        "        cross_entropy = -1/N * sum([math.log(p, 2) for p in training_probabilities])\n",
        "        training_perplexity += math.pow(2, cross_entropy)\n",
        "    print(\"best gamma: \",best_gamma)\n",
        "    print(\"average training perplexity: \", training_perplexity / len(training_text_bigrams))\n",
        "    print(\"average testing perplexity: \", best_testing_average_perplexity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t0LYJtVUJTS"
      },
      "source": [
        "testing_data = pd.read_json(\"https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_test.txt\", lines=True)\n",
        "training_data = pd.read_json(\"https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_train.txt\", lines=True)\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "training_token = tokenize(training_data)\n",
        "testing_token = tokenize(testing_data)\n",
        "\n",
        "vocabulary = get_terms(training_token)\n",
        "\n",
        "training_text = replace_UNK(training_token)\n",
        "testing_text = replace_UNK(testing_token)\n",
        "\n",
        "training_text_bigrams = get_bigrams(training_text)\n",
        "testing_text_bigrams = get_bigrams(testing_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cfva37hL-F9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fa105d41-1d1a-4b96-c28e-57bb2ad1d8ec"
      },
      "source": [
        "counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "get_dict()\n",
        "print(\"PART 1\")\n",
        "get_average_perplexity()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PART 1\n",
            "average training perplexity: 1355.335648828969\n",
            "average testing perplexity:  1640.5500188061494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqG7pwyaULnv"
      },
      "source": [
        "reverse_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "get_reverse_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buaQFBkOUN-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d97a5235-32f5-4d06-b006-897c145539bc"
      },
      "source": [
        "gamma_list = [round(x * 0.05, 2) for x in range(0, 21)]\n",
        "print(\"PART 2\")\n",
        "get_best_average_perplexity()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PART 2\n",
            "best gamma:  0.5\n",
            "average training perplexity:  654.3808701553082\n",
            "average testing perplexity:  744.4107434171606\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}